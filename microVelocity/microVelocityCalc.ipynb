{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10418860-af16-4b7e-8227-e0d6aa7df8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "import os\n",
    "import glob\n",
    "from concurrent import futures           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2d741f-865b-47b4-9114-f01fb35c435e",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKENS = ['FRAX','EURS','XCHF','XAUt','FEI','GUSD','BUSD','sUSD']\n",
    "DECIMALS = [18,2,18,6,18,2,18,18]\n",
    "FIRST_BLOCKS = [11465581,5835474,6622987,9339031,12168368,6302486,8523552,5767935]\n",
    "LAST_BLOCK = 14497033"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4b44f8-04d5-4405-b1ae-4c4f15a60bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculates the holding time for all coins contained in a transaction\n",
    "\n",
    "def holdingTimeCalc(file):\n",
    "\n",
    "    \"\"\"\n",
    "    1. loads the address slices\n",
    "    2. for every address (excluding 0x0-address)\n",
    "        2.1 loads all spending transactions of an address\n",
    "        2.2 for each spending transaction\n",
    "            2.2.1 loads block of transaction\n",
    "            2.2.2 initializes holding time and weights list\n",
    "            2.2.3 applies LIFO-method to calculate the holding time of different tokens contained in a transaction\n",
    "            2.2.4 safes it in a dictionary\n",
    "    \"\"\"\n",
    "\n",
    "    print('{}: initialized'.format(file))\n",
    "    addresses = pd.read_pickle(file)\n",
    "    addresses = list(addresses.keys())\n",
    "\n",
    "    # fromTrxDict = df.groupby('from_address')['transaction_hash_unique'].apply(list).to_dict() # transaction dictionary\n",
    "    # fromTrxIndex = df.groupby('from_address')['index'].apply(list).to_dict()\n",
    "    # toTrxDict = df.groupby('to_address')['transaction_hash_unique'].apply(list).to_dict()\n",
    "    # toTrxIndex = df.groupby('to_address')['index'].apply(list).to_dict()\n",
    "    trxValueOrig = dict(zip(df['transaction_hash_unique'], df['value']))\n",
    "    trxBlock = dict(zip(df['transaction_hash_unique'], df['block_number']))\n",
    "\n",
    "    holdingTimeDict = {}\n",
    "\n",
    "    for address in addresses: \n",
    "        if address != '0x0000000000000000000000000000000000000000':\n",
    "            fromTrxList = list(fromTrxDict[address]) # list of all spending transactions\n",
    "            trxValue = trxValueOrig.copy() \n",
    "            for fromTrx in fromTrxList:\n",
    "                block = trxBlock[fromTrx]\n",
    "                fromTransactions = dict(zip(fromTrxDict[address],fromTrxIndex[address]))\n",
    "                toTransactions = dict(zip(toTrxDict[address],toTrxIndex[address]))\n",
    "                index = fromTransactions[fromTrx]\n",
    "                transactionList = [v for v in toTrxDict[address] if toTransactions[v] < index] # consider only past receiving transactions\n",
    "                fromValue = trxValue[fromTrx]\n",
    "                weights = []\n",
    "                holdingTime = []\n",
    "\n",
    "                for i in reversed(range(len(transactionList))): # itter through receiving transactions in reverse order\n",
    "                    toTrx = transactionList[i]\n",
    "                    if int(fromValue) == 0:\n",
    "                        break\n",
    "                    else:   \n",
    "                        if int(trxValue[toTrx]) >= int(fromValue):\n",
    "                            if int(fromValue) > 0:\n",
    "                                weights.append(int(fromValue))\n",
    "                                holdingTime.append(block - trxBlock[toTrx]) # holding time in number of blocks between trx\n",
    "                            trxValue[toTrx] = str(int(trxValue[toTrx]) - int(fromValue))\n",
    "                            fromValue = 0\n",
    "                        else: \n",
    "                            if int(fromValue) > 0:\n",
    "                                weights.append(int(trxValue[toTrx]))\n",
    "                                holdingTime.append(block - trxBlock[toTrx])\n",
    "                            fromValue = str(int(fromValue) - int(trxValue[toTrx]))\n",
    "                            trxValue[toTrx] = 0  \n",
    "\n",
    "                ht = [holdingTime[x] for x in range(len(holdingTime)) if weights[x] > 0]\n",
    "                w = [weights[x] for x in range(len(weights)) if weights[x] > 0]\n",
    "                if sum(ht) > 0:\n",
    "                    holdingTimeDict[fromTrx] = [w, ht]\n",
    "\n",
    "    f = open(\"holdingTime_{}\".format(file),\"wb\")\n",
    "    pickle.dump(holdingTimeDict,f)\n",
    "    f.close()  \n",
    "    print('{}: sucess'.format(file))\n",
    "\n",
    "for i in range(len(TOKENS)):\n",
    "    TOKEN = TOKENS[i]\n",
    "    FIRST_BLOCK = FIRST_BLOCKS[i]\n",
    "    \n",
    "    # load the test transaction data\n",
    "    TransactionsFull = pd.read_csv('{}_token_transfers.csv'.format(TOKEN))\n",
    "\n",
    "    # load the supply data (supply at the last block per day - from etherscan)\n",
    "    Supply = pd.read_csv('{}_supply.csv'.format(TOKEN))\n",
    "\n",
    "    # data cleaning\n",
    "    df = TransactionsFull.sort_values(by=['block_number', 'log_index']).reset_index(drop=True)\n",
    "    \n",
    "    # exclude transactions where value is 0 or from_address == to_address\n",
    "    df = pd.DataFrame(df.loc[((df['value'] != 0) | (df['value'] != '0')) & (df['from_address'] != df['to_address'])])\n",
    "\n",
    "    # create new transaction_hash column by appending log_index to make every hash unique\n",
    "    transactionHash = df.transaction_hash.to_list()\n",
    "    logIndex = df.log_index.to_list()\n",
    "    transactionHashUnique = []\n",
    "    for i in range(0,len(transactionHash)):\n",
    "        t = transactionHash[i]\n",
    "        l = logIndex[i]\n",
    "        u = str(t)+'__'+str(l)\n",
    "        transactionHashUnique.append(u)\n",
    "\n",
    "    df['transaction_hash_unique'] = transactionHashUnique\n",
    "    df['index'] = [x for x in range(len(df))]\n",
    "    df.to_csv('{}_token_transfers_new'.format(TOKEN))\n",
    "    fromTrxDict = df.groupby('from_address')['transaction_hash_unique'].apply(list).to_dict()\n",
    "    fromTrxIndex = df.groupby('from_address')['index'].apply(list).to_dict()\n",
    "    toTrxDict = df.groupby('to_address')['transaction_hash_unique'].apply(list).to_dict()\n",
    "    toTrxIndex = df.groupby('to_address')['index'].apply(list).to_dict()\n",
    "\n",
    "    addresses = list(fromTrxDict.keys())\n",
    "    c = 1\n",
    "    for a in np.array_split(addresses, 40):\n",
    "        a_subset = {key: value for key, value in fromTrxDict.items() if key in a}\n",
    "        f = open(\"sliced_accounts_{}_{}.pkl\".format(TOKEN,c),\"wb\")\n",
    "        pickle.dump(a_subset,f)\n",
    "        f.close() \n",
    "        c += 1\n",
    "        \n",
    "    with futures.ProcessPoolExecutor(max_workers=40) as ex:\n",
    "        for slice in glob.glob('sliced_accounts_{}_*.pkl'.format(TOKEN)):\n",
    "            ex.submit(holdingTimeCalc, slice)\n",
    "   \n",
    "    d0 = pd.read_pickle('holdingTime_sliced_accounts_{}_1.pkl'.format(TOKEN))\n",
    "    for _filename in glob.glob('holdingTime_sliced_accounts_{}_*.pkl'.format(TOKEN)):\n",
    "        d1 = pd.read_pickle(_filename)\n",
    "        d0.update(d1) \n",
    "\n",
    "    f = open('{}_holdingTimes.pkl'.format(TOKEN),'wb')\n",
    "    pickle.dump(d0,f)\n",
    "    f.close()\n",
    "    print('{}_holdingTimes.pkl: sucess'.format(TOKEN)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca70b7b7-0fd3-4a8a-955f-7fa172001471",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculates the MicroVelocity from the holding time data\n",
    "\n",
    "def microVelocityCalc(file):\n",
    "    \n",
    "    \"\"\"\n",
    "    1. reads in the sliced addresses\n",
    "    2. for every address\n",
    "        2.1 initiates 0-array of length (last block - first block)\n",
    "        2.2 for every transaction\n",
    "            2.2.1 calculates microVelocity\n",
    "                    - sum for the comparison with Fisher's eq.\n",
    "                    - mean for analysis\n",
    "                    - snapshots at the last block per day for analysis\n",
    "    \"\"\"\n",
    "\n",
    "    ts = pd.read_csv('block_timestamps_complete.csv', parse_dates=['timestamp'])\n",
    "    ts = ts.loc[(ts.block_number >= FIRST_BLOCK) & (ts.block_number <= LAST_BLOCK)]\n",
    "    ts_w = pd.DataFrame(ts[['timestamp','block_number']].groupby(pd.Grouper(key='timestamp', axis=0, freq='D')).first()).reset_index()\n",
    "\n",
    "    first = pd.DataFrame(ts[['timestamp','block_number']].groupby(pd.Grouper(key='timestamp', axis=0, freq='D')).first()).reset_index()\n",
    "    first = first['block_number'].to_list()\n",
    "    last = pd.DataFrame(ts[['timestamp','block_number']].groupby(pd.Grouper(key='timestamp', axis=0, freq='D')).last()).reset_index()\n",
    "    last = last['block_number'].to_list()\n",
    "\n",
    "    trxBlock = dict(zip(df.transaction_hash_unique, df.block_number)) # dictionary with {transaction_hash : block_number}\n",
    "    SupplyDict = dict(zip(Supply.block_number, Supply.supply)) # dictionary with {block_number : supply}\n",
    "    holdingTime = pd.read_pickle('{}_holdingTimes.pkl'.format(TOKEN)) # dictionary with {transaction_hash : avg_ht}\n",
    "    fromTrxDict = df.groupby('from_address')['transaction_hash_unique'].apply(list).to_dict()\n",
    "    velocity_sum = {}\n",
    "    velocity_mean = {}\n",
    "    velocity_last = {}\n",
    "    \n",
    "    addresses = pd.read_pickle(file)\n",
    "    addresses = list(addresses.keys())\n",
    "    print('{}_initialized'.format(file))\n",
    "    \n",
    "    for address in addresses:\n",
    "        ind_velocity = np.zeros(LAST_BLOCK+1)\n",
    "        transactions = list(fromTrxDict[address])\n",
    "        for trx in transactions: # take all transactions\n",
    "            if trx in holdingTime.keys():\n",
    "                am = holdingTime[trx][0] # number of coins with holding time tau\n",
    "                ht = holdingTime[trx][1] # holding time in number of blocks\n",
    "                if sum(ht) > 0:\n",
    "                    height = trxBlock[trx] # block_number\n",
    "                    for a in range(len(am)):\n",
    "                        ht_ = ht[a]\n",
    "                        am_ = am[a]\n",
    "                        if ht_ > 0:\n",
    "                            for t in range(int(round(height-ht_)),height): # loop over the blocks\n",
    "                                ind_velocity[t] += int(am_) / ht_ * 1/int(SupplyDict[t])\n",
    "        s = [sum(ind_velocity[first[i]:last[i]]) for i in range(len(first))]\n",
    "        m = [s[i]/(last[i]-first[i]) for i in range(len(first))]\n",
    "        l = [ind_velocity[i] for i in last] # snapshots at the last block per day\n",
    "        velocity_sum[address] = s\n",
    "        velocity_mean[address] = m \n",
    "        velocity_last[address] = l\n",
    "        \n",
    "    f = open(\"microVelocity_sum_{}\".format(file),\"wb\")\n",
    "    pickle.dump(velocity_sum,f)\n",
    "    f.close()  \n",
    "    f = open(\"microVelocity_mean_{}\".format(file),\"wb\")\n",
    "    pickle.dump(velocity_sum,f)\n",
    "    f.close()  \n",
    "    f = open(\"microVelocity_last_{}\".format(file),\"wb\")\n",
    "    pickle.dump(velocity_last,f)\n",
    "    f.close()  \n",
    "    print('{}: sucess'.format(file))\n",
    "\n",
    "for i in range(len(TOKENS)):\n",
    "    TOKEN = TOKENS[i]\n",
    "    FIRST_BLOCK = FIRST_BLOCKS[i]\n",
    "    \n",
    "    # load the test transaction data\n",
    "    TransactionsFull = pd.read_csv('{}_token_transfers_new.csv'.format(TOKEN))\n",
    "\n",
    "    # load the supply data (supply at the last block per day - from etherscan)\n",
    "    Supply = pd.read_csv('{}_supply.csv'.format(TOKEN))\n",
    "    \n",
    "    with futures.ProcessPoolExecutor(max_workers=40) as ex:\n",
    "        for slice in glob.glob('sliced_accounts_{}_*.pkl'.format(TOKEN)):\n",
    "            ex.submit(microVelocityCalc, slice)\n",
    "\n",
    "    d0 = pd.read_pickle('microVelocity_sum_sliced_accounts_{}_1.pkl'.format(TOKEN))\n",
    "    for file in glob.glob('microVelocity_sum_sliced_accounts_{}_*.pkl'.format(TOKEN)):\n",
    "        d1 = pd.read_pickle(file)\n",
    "        d0.update(d1) \n",
    "\n",
    "    f = open('{}_microVelocities_sum.pkl'.format(TOKEN),'wb')\n",
    "    pickle.dump(d0,f)\n",
    "    f.close()\n",
    "\n",
    "    d0 = pd.read_pickle('microVelocity_mean_sliced_accounts_{}_1.pkl'.format(TOKEN))\n",
    "    for file in glob.glob('microVelocity_mean_sliced_accounts_{}_*.pkl'.format(TOKEN)):\n",
    "        d1 = pd.read_pickle(file)\n",
    "        d0.update(d1) \n",
    "\n",
    "    f = open('{}_microVelocities_mean.pkl'.format(TOKEN),'wb')\n",
    "    pickle.dump(d0,f)\n",
    "    f.close()\n",
    "\n",
    "    d0 = pd.read_pickle('microVelocity_last_sliced_accounts_{}_1.pkl'.format(TOKEN))\n",
    "    for file in glob.glob('microVelocity_last_sliced_accounts_{}_*.pkl'.format(TOKEN)):\n",
    "        d1 = pd.read_pickle(file)\n",
    "        d0.update(d1) \n",
    "\n",
    "    f = open('{}_microVelocities_snapshots.pkl'.format(TOKEN),'wb')\n",
    "    pickle.dump(d0,f)\n",
    "    f.close()\n",
    "    print('{}_microVelocities_snapshots: sucess'.format(TOKEN))\n",
    "\n",
    "    for file in glob.glob('sliced_accounts_{}_*.pkl'.format(TOKEN)):\n",
    "        os.remove(file)\n",
    "    for file in glob.glob('microVelocity_sum_sliced_accounts_{}_*.pkl'.format(TOKEN)):\n",
    "        os.remove(file)\n",
    "    for file in glob.glob('microVelocity_mean_sliced_accounts_{}_*.pkl'.format(TOKEN)):\n",
    "        os.remove(file)\n",
    "    for file in glob.glob('microVelocity_last_sliced_accounts_{}_*.pkl'.format(TOKEN)):\n",
    "        os.remove(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cedd94f-c19a-4640-90a2-45694267e964",
   "metadata": {},
   "outputs": [],
   "source": [
    "# turns pickle file into data frame for convenience\n",
    "ts = pd.read_csv('block_timestamps_complete.csv', parse_dates=['timestamp'])\n",
    "\n",
    "for i in range(len(TOKENS)):\n",
    "    TOKEN = TOKENS[i]\n",
    "    FIRST_BLOCK = FIRST_BLOCKS[i]\n",
    "    dct = pd.read_pickle('{}_microVelocities_last.pkl'.format(TOKEN))\n",
    "    df = pd.DataFrame.from_dict(dct, orient='index').T\n",
    "    df['total'] = df.sum(axis=1)\n",
    "    ts2 = pd.DataFrame(ts.loc[(ts.block_number >= FIRST_BLOCK) & (ts.block_number <= LAST_BLOCK)])\n",
    "    ts2 = pd.DataFrame(ts2[['timestamp','block_number']].groupby(pd.Grouper(key='timestamp', axis=0, freq='D')).last()).reset_index()\n",
    "    timestamp = ts2.timestamp.to_list()\n",
    "    df['timestamp'] = timestamp\n",
    "    df.to_csv('{}_microVelocities_last.csv'.format(TOKEN),index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
